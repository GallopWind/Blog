# 泛化 数据 训练 特征 

## LR

$$
y:实际值\quad y^{'}:预测值\quad \theta^{T}:模型参数向量\quad X:特征向量\ \\
m:样本数量\quad n:特征数量\quad \eta:学习率 \\
\begin{gather}
y^{'}=h_{\theta}(X)=\theta^{T}\cdot X \tag{1} \\
MSE(h_{\theta},X)=\frac{1}{m}\sum_{i}^{m}\big(\theta^T\cdot X^i-y^i\big)^2 \tag{2} \\
标准方程法:\ \theta^{'}=(X^T\cdot X)^{-1}\cdot X^T\cdot y  \tag{3} \\
\nabla_{\theta}MSE(\theta) = \frac{2}{m}X^T\cdot(X\cdot\theta-y) \tag{4} \\
梯度下降法:\theta^{k+1}=\theta^{k}-\eta\nabla_{\theta}MSE(\theta) \tag{5} \\

\end{gather}
$$

### 标准方程法

算法复杂度 O(n^{2.4}, m), 对于样本多, 优; 对于特征多, 劣.

### 梯度下降法
关键: 运算速度(小批量下降), 局部最优(随机初始化), 收敛最优(针对随机梯度).

批量梯度下降法: 使用全部样本, 可以收敛最优, 但样本较多时计算较慢.

随机梯度下降: 每次随机使用一个样本进行梯度下降. 优点: 速度快, 可以核外运算, 有助于跳出局部最优. 缺点: 训练过程中下降不稳定, 需要使用退火算法, 收敛速度慢.

小批量梯度下降: 每次使用一批样本进行训练. 二者的折中.

## 归一化, 正则化

### 归一化

将输入数据进行中心, 方差归一化处理

意义:

- 正则化需要模型参数处于同一尺寸
- 梯度下降需要下降参数区间位于一定范围内

### 正则化

在训练时对模型参数进行惩罚, 依据不同的惩罚方式, 使模型平滑稀疏, 提高泛化性能
$$
\alpha:正则化系数 \quad \theta:模型参数 \quad r:混合比例 \\
\begin{align}
L1范数&:\alpha\sum|\theta| \\
L2范数&:\alpha\sum\theta^2 \\
弹性网络&:r\alpha\sum|\theta|+\frac{1-r}{2}\alpha\sum\theta^2
\end{align}
$$

#### L1正则化

由于导数不随$\theta$ 的值而变化, 因此L1正则化在"谷底"区域更倾向于将$\theta$ 正则化至0, 使模型稀疏.

注意由于在0处不可导, 需要指定该处梯度为0

#### L2正则化

常用正则化

#### 弹性网络正则化

根据超参数$r$ 来调整.

#### 其他正则化

- 提前停止法, 当学习曲线下降再上升一段时间后, 选择最低处的模型作为训练结果.

## SVM

$$
\alpha_i \geq 0,松弛变量 \quad 为简写,向量点积不再转置\\
\begin{gather}
\gamma_{i}=y_{i}\left(\frac{\boldsymbol{w}}{\|\boldsymbol{w}\|} \cdot \boldsymbol{x}_{i}+\frac{b}{\|\boldsymbol{w}\|}\right) \tag1 \\
y_{i}\left(\frac{\boldsymbol{w}}{\|\boldsymbol{w}\|} \cdot \boldsymbol{x}_{i}+\frac{b}{\|\boldsymbol{w}\|}\right) \geq \gamma \tag2 \\
\boldsymbol{w'}=\frac{\boldsymbol{w}}{\|\boldsymbol{w}\| \gamma} \quad b'=\frac{b}{\|\boldsymbol{w}\| \gamma} \tag3 \\
y_{i}\left(\boldsymbol{w'} \cdot \boldsymbol{x}_{i}+b'\right)-1 \geq 0 \tag4 \\ \\
\boldsymbol{w'} \to \boldsymbol{w}, \quad b'\to b \\
\min_{\boldsymbol{w},b}\ L(\boldsymbol{w}, b, \boldsymbol{\alpha})=\frac{1}{2}\|\boldsymbol{w}\|^{2}-\sum_{i=1}^{N} \alpha_{i}\left(y_{i}\left(\boldsymbol{w} \cdot \boldsymbol{x}_{i}+b\right)-1-\beta_i^2\right) \tag{5.1}
\end{gather}
$$




$$
\begin{gather}
\min_{\boldsymbol{w},b}\ L(\boldsymbol{w}, b, \boldsymbol{\alpha})=\frac{1}{2}\|\boldsymbol{w}\|^{2}-\sum_{i=1}^{N} \alpha_{i}\left(y_{i}\left(\boldsymbol{w} \cdot \boldsymbol{x}_{i}+b\right)-1\right) \tag{5.2} \\
\theta(w) = \max _{\alpha_{i} \geq 0} L(\boldsymbol{w}, b, \boldsymbol{\alpha})=
\left\{\begin{matrix} 
  \frac{1}{2}||\boldsymbol{w}^2||&,\boldsymbol{x}\in 可行区域 \\
  +\infty&,\boldsymbol{x}\in不可行区域
\end{matrix}\right. \\ \\
\min _{\boldsymbol{w}, b} \theta(\boldsymbol{w})=\min _{\boldsymbol{w}, b} \max _{\alpha_{i} \geq 0} L(\boldsymbol{w}, b, \boldsymbol{\alpha})=p^{*} \tag6 \\
\max _{\alpha_{i} \geq 0} \min _{\boldsymbol{w}, b} L(\boldsymbol{w}, b, \boldsymbol{\alpha})=d^{*} 
\tag7 \\
\left\{\begin{array}{l}
\alpha_{i} \geq 0 \\
y_{i}\left(\boldsymbol{w}_{i} \cdot \boldsymbol{x}_{i}+b\right)-1 \geq 0 \\
\alpha_{i}\left(y_{i}\left(\boldsymbol{w}_{i} \cdot \boldsymbol{x}_{i}+b\right)-1\right)=0
\end{array}\right. \tag8 \\ \\
\arg _{\boldsymbol{w}, b}L(\boldsymbol{w}, b, \boldsymbol{\alpha}) = 
\left\{\begin{array}{}
\boldsymbol{w}=\sum_{i=1}^{N} \alpha_{i} y_{i} \boldsymbol{x}_{i} \\
\sum_{i=1}^{N} \alpha_{i} y_{i}=0
\end{array}\right. \tag9
\end{gather}
$$




$$
\begin{gather}
\begin{array}{c}
L(\boldsymbol{w}, b, \boldsymbol{\alpha})=\frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j}\left(\boldsymbol{x}_{i} \cdot \boldsymbol{x}_{j}\right)-\sum_{i=1}^{N} \alpha_{i} y_{i}\left(\left(\sum_{j=1}^{N} \alpha_{j} y_{j} \boldsymbol{x}_{j}\right) \cdot \boldsymbol{x}_{i}+b\right)+\sum_{i=1}^{N} \alpha_{i} \\
=-\frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j}\left(\boldsymbol{x}_{i} \cdot \boldsymbol{x}_{j}\right)+\sum_{i=1}^{N} \alpha_{i}
\end{array} \tag{10} \\
\min _{\boldsymbol{\alpha}} \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j}\left(\boldsymbol{x}_{i} \cdot \boldsymbol{x}_{j}\right)-\sum_{i=1}^{N} \alpha_{i} \tag{11} \\
\end{gather}
$$



$$
\begin{gather}
\boldsymbol{x} \to \phi(\boldsymbol{x}) \\
\left(\boldsymbol{x}_{i} \cdot \boldsymbol{x}_{j}\right) \to \left(\phi(\boldsymbol{x}_{i}) \cdot \phi(\boldsymbol{x}_{j}\right)) \\ \\
\begin{aligned}
线形核函数 &:& K(a,b) &= a\cdot b \\
多项式核函数 &:& K(a,b) &= (\gamma a^T\cdot b +r)^d \\
RBF核函数 &:& K(a,b) &= exp(-\gamma||a-b||^2) \\
Sigmoid核函数 &:& K(a,b) &=tanh(\gamma a^T \cdot b +r)
\end{aligned} \\ \\
例:二阶核函数 \phi 
\left(\begin{array}{1} x_1 \\ x_2
\end{array}\right) = \left(\begin{gathered}
x_1^2 \\ \sqrt{2}x_1x_2 \\ x_2
\end{gathered}\right) \\
K(a,b) = \phi(a)\phi(b) = (a \cdot b)^2 \\ \\
y' \to y' = \boldsymbol{w} \cdot \phi(\boldsymbol{x})+b  \tag{12}\\
\begin{aligned}
y' &= \boldsymbol{w} \cdot \phi(\boldsymbol{x})+b \\
&= \sum_{i=1}^{N} \alpha_{i} y_{i} \phi(\boldsymbol{x}_{i}) \cdot \phi(\boldsymbol{x})+b \\
&= \sum_{i=1}^{N} \alpha_{i} y_{i}K(\boldsymbol{x}_{i},\boldsymbol{x}) +b
\end{aligned} \tag{13}
\end{gather}
$$


